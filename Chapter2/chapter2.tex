%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Adjoint based shape optimization}

\ifpdf
    \graphicspath{{Chapter2/Figs/Raster/}{Chapter2/Figs/PDF/}{Chapter2/Figs/}}
\else
    \graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}
\fi

Here an later we use the following denominations for scalar products:

\begin{subequations}
\begin{align}
          \left< \cdot, \cdot \right> = \int_{\Omega} dx (\cdot, \cdot ), \\
          \left\{ \cdot, \cdot \right\} = \int_{T} dt (\cdot, \cdot ), \\
          \left[ \cdot, \cdot  \right] = \int_{\Omega} dx\int_{T} dt (\cdot, \cdot )
\end{align}
\end{subequations}


\section{Pedagogical one dimensional optimization}

To start with, we consider two one-dimensional optimization problems: parameter-based unsteady Burgers equation, and Helmholtz equation with adjustable shape. These examples are aimed to give an insight into construction of adjoint systems and how to obtain cost function gradient with respect to different changes in the initial problem.

\subsection{Burgers equation: sensitivity to control parameters}

Burgers equation (\ref{eq:BurgEq}), is a nonlinear partial differential equation describing one-dimensional viscous fluid. We consider it to be defined in closed spatial domain $x \in [A,B]$ and for time interval $t \in [0, T]$.

\begin{equation}
\label{eq:BurgEq}
    u_t + u u_x - \nu u_{xx} = 0
\end{equation}

Initial condition $u(x,0)$ is given, as well as two Dirichlet boundary conditions:

\begin{subequations}
\label{eq:BurgEqBCs}
\begin{align}
    u(x,0) = u_0(x), \\
    u(A,t) = f(t), \\
    u(B,t) = 1.
\end{align}
\end{subequations}

Here the left boundary condition, $f(t)$, will act as a control parameter. We would like to optimize it in order to minimize a cost function, for instance, time-averaged viscous dissipation:

\begin{equation}
\mathcal{J} = \left[ \nu \left( \frac{\partial u}{\partial x} \right)^2 \right]
\end{equation}

Following \cite{CossuIntr}, in this example we will treat initial and boundary conditions (\ref{eq:BurgEqBCs}) as given constraints, i.e. in the same as the Burgers equation itself. Another approach, which leads to the same result, is to apply given boundary conditions after the augmented cost function is formed.

The first step is to form a so-called Lagrangian of the system, by adding all state equations multiplied by Lagrange variables to the cost function. This gives:

\begin{subequations}
\begin{align}
    \label{eq:BurgEqAdj0}
    \mathcal{L} = \mathcal{J} - \left[ \lambda, u_t + u u_x - \nu u_{xx} \right] -\\
    - \left< b, u(x,0) - u_0 \right> -  \left\{ C_{\alpha}, u(A,t) - f(t) \right\}  -  \left\{ C_{\beta}, u(B,t) - 1 \right\}
\end{align}
\end{subequations}

Here $\lambda$ is an Lagrange state multiplier, $b$ and $C_{\alpha}, C_{\beta}$ are Lagrange multipliers for initial and boundary conditions, respectively. One can see, that partial derivatives of $\mathcal{L}$ with respect to $\lambda$ produce initial state equation, and initial and boundary equations while differentiation with respect to $b, C_{\alpha}, C_{\beta}$. To construct the adjoint operator, we integrate the differential form in (\ref{eq:BurgEqAdj0}) by parts and apply divergence theorem, which results in:

\begin{subequations}
\label{eq:BurgEqAdj}
\begin{align}
    \label{eq:BurgEqAdj1}
    \mathcal{L} = \mathcal{J} - \left[ -\lambda_t - \frac{1}{2} u \lambda_x - \nu \lambda_{xx}, u \right] -\\
    - \left< \lambda, u \right>^T_0 - \left\{\frac{1}{2} \lambda u - \nu \lambda_x, u \right\}^B_A + \left\{ \nu \lambda, u_x \right\}^B_A - \\
    - \left< b, u(x,0) - u_0 \right> -  \left\{ C_{\alpha}, u(A,t) - f(t) \right\}  -  \left\{ C_{\beta}, u(B,t) - 1 \right\}
\end{align}
\end{subequations}

Now, the total variation of the Lagrangian $\delta \mathcal{L}$ can be written as:

\begin{subequations}
\begin{align}
    \label{eq:BurgEqAdjVariation}
    0 = \delta \mathcal{L} = \delta \mathcal{J} + \left[ \lambda_t + u \lambda_x + \nu \lambda_{xx}, \delta u \right] -\\
    - \left< \lambda(x,T), \delta u(x,T) \right> + \left< \lambda(x,0), \delta u(x,0) \right> - \\
    - \left\{ \lambda u - \nu \lambda_x, \delta u \right\}^B_A + \left\{ \nu \lambda, (\delta u)_x \right\}^B_A - \\
    - \left< b, \delta u(x,0) \right> -  \left\{ C_{\alpha}, \delta u(A,t) \right\}  -  \left\{ C_{\beta}, \delta u(B,t) \right\}
\end{align}
\end{subequations}

Unspecified variation of the cost function, $\delta \mathcal{J}$, is:

\begin{equation}
\delta \mathcal{J} = \left[ - 2 \nu u_xx, \delta u \right] + \left\{ 2 \nu u_x, \delta u  \right\}^B_A
\end{equation}

Finally, gathering terms with the same variation quantity, extracts the adjoint set of equations. The $\left[ \cdot , \delta u \right] $ terms give the adjoint state equation:

\begin{equation}
\lambda_t + u \lambda_x + \nu \lambda_{xx} = 2 \nu u_{xx}
\end{equation}

Worth mentioning that despite this equation is quite similar to the original Burgers equation - it has temporal derivative, convective, and a viscous term, it is linear on $\lambda$, and has a source function. Moreover, viscosity has a different sign in comparison to the direct problem, thus the adjoint problem will be well-posed only if propagates back in time, starting with 'initial data' at the final time $t = T$ \cite{Giles2000}. This means, that integration in time should follow the direct solution backwards.

Gathering the rest terms provides the necessary adjoint boundary and initial conditions:

\begin{subequations}
\begin{align}
    \label{eq:BurgEqAdjBCt0}
    & \delta u(x,0): \ \ \lambda - b = 0 & \Rightarrow \ \ & b = \lambda(x,0),\\
    \label{eq:BurgEqAdjBCtT}
    & \delta u(x,T): \ \ \lambda = 0 & \Rightarrow \ \ & \lambda(x,T) = 0 ,\\
    & \delta u_x(A,t): \ \ \nu \lambda = 0 & \Rightarrow \ \ & \lambda(A,t) = 0, \\
    & \delta u_x(B,t): \ \ \nu \lambda = 0 & \Rightarrow \ \ & \lambda(B,t) = 0, \\
    & \delta u(A,t): \ \ 2 \nu u_x - \lambda u - \nu \lambda_x + C_{\alpha} = 0, \\
    & \delta u(B,t): \ \ 2 \nu u_x - \lambda u - \nu \lambda_x - C_{\beta} = 0
\end{align}
\end{subequations}

The first two conditions (\ref{eq:BurgEqAdjBCt0}) and (\ref{eq:BurgEqAdjBCtT}) are sensitivity to the initial condition $u(x,0) = u_0$ and the adjoint initial condition, which says that $\lambda(x,T) = 0$; the following two are the adjoint boundary conditions, and the last two give us the values of $C_{\alpha}, C_{\beta}$ at $x = A$ and $x = B$, respectively.

Finally, from (\ref{eq:BurgEqAdj}) it is clear, that the sensitivity with respect to the boundary condition $u(A,t) = f(t)$ equals to:

\begin{equation}
\frac{\partial \mathcal{L}}{\partial f} \delta f = \left\{C_{\alpha} , \delta f(t)  \right\}
\end{equation}

And consequently, the gradient of the cost function with respect to $f(t)$ becomes:

\begin{equation}
\nabla \mathcal{J}[f] = \left\{ C_{\alpha} \right\} = \left\{ -2 \nu u_x (A,t) + \nu \lambda_x (A,t) \right\}
\end{equation}

\subsection{Shape optimization of linear acoustic problem}

Now we consider the one-dimensional wave equation in frequency domain, or the Helmholtz equation:

\begin{equation}
\label{eq:HelmholtzEq}
    P_{xx} - s^2 P = 0
\end{equation}

This is a spectral problem, where $P_k$ and $s_k$ represent a $k-$th eigenfunction (or natural mode) and an eigenvalue of the given system. The general solution can be written as: $P_k = a_k sin(s_k x) + b_k cos(s_k x)$, where $a_k, b_k$ are some constants subject to boundary conditions.

We aim to perform shape optimization to minimize eigenvalues of the problem; namely, since the problem is 1D, we will find the derivative of the eigenvalue with respect to the position of the left boundary. Without loss of generality, let the initial domain be $x \in [0,1]$, and let us call $V$ the boundary displacement field at $x = 0$, such that the deformed domain is $x \in [tV, 1]$. Here $t$ is a parameter which governs the amplitude of the displacement.  Additionally, we can recast the cost function to become $\mathcal{J} = \sigma = s^2$.

The augmented cost function for the problem is defined by:

\begin{equation}
\label{eq:HelmhAdj}
    \mathcal{L} = \mathcal{J} + \left<\lambda, P_{xx} - \sigma P \right>
\end{equation}

Note, that we can construct the Lagrangian without knowing the boundary conditions; in contrast to the previous section, we will apply boundary conditions after the Lagrangian variation is obtained.

Integration by parts and taking the variation of the augmented cost function (\ref{eq:HelmhAdj}) leads to:

\begin{subequations}
\begin{align}
    0 = \delta \mathcal{L} = \delta \sigma + \left<\lambda_{xx} - \sigma \lambda , \delta P \right> - \delta \sigma \left< \lambda, P \right> - \\
    - \left( \lambda_x , \delta P \right) \rvert^1_0 + \left( \lambda , \delta P_x \right) \rvert^1_0
\end{align}
\end{subequations}

We can extract two relevant equations from this expression: first, gathering the terms of $\left<\cdot , \delta P \right> $ and the terms proportional to $\delta \sigma$. These yield:

\begin{subequations}
\begin{align}
    & \delta P: \ \  \lambda_{xx} - \sigma \lambda = 0, \\
    \label{eq:HelmAdjNormCond}
    & \delta \sigma: \ \ 1 - \left<\lambda, P \right> = 0.
\end{align}
\end{subequations}

The first equation is a differential equation of the adjoint state; the second one is a normalization condition.

Now, we have two boundary terms left, and further derivation of the adjoint boundary condition will depend on the choice of the direct formulation. Here we consider three different cases: dual Dirichlet boundary conditions, dual Neumann boundaries, and a Dirichlet plus Robin boundary conditions.

\textbf{Case 1}. Dual Dirichlet boundary conditions.

Let us fix the boundary values $P(0) = P(1) = 0$, therefore the variation $\delta P$ vanishes on the boundaries: $\delta P(0) = \delta P(1) = 0$. This leads to the following representation of the Lagrangian variation:

\begin{equation}
    \delta \mathcal{L} = \left( \lambda , \delta P_x \right) \rvert^1_0 = 0
\end{equation}

Remaining terms are proportional to the unknown $\delta P_x$ thus vanish only if $\lambda(0) = \lambda(1) = 0$, which provides us with the necessary boundary conditions. The problem then becomes self-adjoint, since both the state equation and the boundary conditions are the same for the direct and the adjoint state.

\textbf{Case 2}. Dual Neumann boundary conditions.

Almost the same results can be obtained while considering two Neumann boundary conditions: $P_x(0) = P_x(1) = 0$, except that now another boundary term disappears leaving us with:

\begin{equation}
    \delta \mathcal{L} = -\left( \lambda_x , \delta P \right) \rvert^1_0 = 0
\end{equation}

Therefore we should impose two homogeneous Neumann boundary conditions on $\lambda$ in order to eliminate unknown variations of $\delta P$. As a consequence, the problem is still self-adjoint with $\lambda_x(0) = \lambda_x(1) = 0$.

\textbf{Case 3}. Dirichlet and Robin boundary conditions.

Finally, let us introduce a mixed boundary condition at $x = 0$ and the Dirichlet at $x = 1$. Namely, $k P(0) - P_x (0) = 0$ and $P(1) = 0$, where $k$ is some given constant. First, the boundary term proportional to $\delta P(1)$ is zero, and the second boundary term at $x=1$ can be zero only if $\lambda(1) = 0$. Then, the remaining expression equals to:

\begin{equation}
    \delta \mathcal{L} = \left( \lambda_x(0) , \delta P(0) \right) - \left( \lambda(0) , \delta P_x (0) \right)  = 0
\end{equation}

The variation of the Robin boundary condition allows us to express $\delta P(0)$ or $\delta P_x(0)$ through each other, i.e. $\delta k P(0) = \delta P_x(0)$ and apply this to the boundary terms:

\begin{equation}
    \delta \mathcal{L} = \left( \lambda_x(0) , \delta P(0) \right) - \left( \lambda(0) , k \delta P (0) \right)  = \left( \lambda_x(0) - k \lambda(0) , \delta P (0) \right) = 0
\end{equation}

This gives us the Robin boundary condition on $\lambda$, and as we can see, the direct and the adjoint problems are equivalent as well as in the previous cases. These results can be reformulated as follows: the problem (\ref{eq:HelmholtzEq}) with the squared eigenvalue as the objective function is self-adjoint, and in order to obtain the adjoint field $\lambda$ one should apply the normalization condition (\ref{eq:HelmAdjNormCond}).

Following \cite{Schmidt2010}, but with the simplification to 1D case, we can present the full variation (or the material derivative) as a sum of local and spatial derivatives:

\begin{equation}
    dP[V](x) = \left.\frac{d}{dt} \right\rvert_{t=0} P_t (x_t) = \left.\frac{\partial}{\partial t}\right\rvert_{t=0} P_t + \left.\frac{\partial P}{\partial x}\right\rvert_{t=0} \frac{d x_t}{d t}
\end{equation}

Here $[V]$ stands for the derivative with respect to shape, and $_t$ index corresponds to the shifted boundary point or the solution in the changed domain. According to the definition $x_t = tV$, so the total derivative becomes:

\begin{equation}
dP[V](x) = P'[V] + V P_x 
\end{equation}

Following the same logic, the material derivative of the spatial derivative $P_x$ can be written as:

\begin{equation}
dP_x[V](x) = \left(P'[V]\right)' + V P_{xx} 
\end{equation}

Now, to cover all three cases at once, let us derive the shape derivative of the objective function for Case 3. First, from the Robin boundary condition and using the expressions for the material derivatives:

\begin{equation}
d(P_x - k P) = 0 \ \ \Rightarrow \ \ \left(P'[V]\right)' = k P'[V] + k V P_x -  V P_{xx}
\end{equation}

And the Lagrangian shape derivative

\begin{subequations}
\begin{align}
\mathcal{L}'[V] = \left( \lambda_x , P'[V] \right)_{x = 0} - \left( \lambda , \left(P'[V]\right)'  \right)_{x = 0} = \\
\label{eq:HelmhShapeDerRobin}
\left( \lambda_x - k \lambda, P'[V] \right)_{x = 0}- \left( \lambda , k V P_x - VP_{xx}  \right)_{x = 0}
\end{align}
\end{subequations}

The first bracket in (\ref{eq:HelmhShapeDerRobin}) is zero due to the boundary condition; since the system is self-adjoint, we can use $P(x)$ instead of $\lambda(x)$ (keeping in mind the normalization condition), and the shape derivative finally is:

\begin{equation}
\label{eq:HelmholtzEqGrad}
    d \sigma [V] = V \left( P P_{xx} - k P P_x \right)_{x = 0}
\end{equation}

Summing up, once the solution of the spectral problem (\ref{eq:HelmholtzEq}) is obtained, to calculate the shape derivative of the cost function $\mathcal{J} = s^2$ with respect to shape one needs to normalize the eigenfunction such way that $1 = \left<P,P\right>$, and then the desired gradient equals to (\ref{eq:HelmholtzEqGrad}).

\clearpage

\section{Adjoints for incompressible Navier-Stokes equation}
\subsection{Governing equations}
\subsection{Adjoint boundary conditions}
\subsection{Shape derivatives and Hadamard structure}
\subsection{Time dependent flow optimization}

\section{Adjoints for acoustic problem}
\subsection{Governing equations}
\subsection{Adjoint boundary conditions}
\subsection{Shape derivatives and Hadamard structure}

\section{Taylor testing for adjoint gradient}
